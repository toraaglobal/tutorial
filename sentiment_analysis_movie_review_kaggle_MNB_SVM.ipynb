{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_movie_review_kaggle_MNB-SVM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toraaglobal/tutorial/blob/text_mining/sentiment_analysis_movie_review_kaggle_MNB_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5WaSIefyLjP",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis of Movie Reviews\n",
        "**Comparing MNB and SVMs for Kaggle Sentiment Classification**\n",
        "***\n",
        "\n",
        "The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee . In their work on sentiment treebanks, Socher et al.used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.\n",
        "\n",
        "\n",
        "[Data Source](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview)\n",
        "\n",
        "**Evaluation**\n",
        "Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly) for every parsed phrase. The sentiment labels are:\n",
        "\n",
        "* 0 - negative\n",
        "* 1 - somewhat negative\n",
        "* 2 - neutral\n",
        "* 3 - somewhat positive\n",
        "* 4 - positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex66mmFf1dZA",
        "colab_type": "text"
      },
      "source": [
        "**Import Packages**\n",
        "The following python packages are used for this analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWkwyZ_xmrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ba2da433-624d-4b68-e677-9dcd1ae87f7b"
      },
      "source": [
        "## Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "\n",
        "## vectoriation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "## model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "## model evaluation\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix  # confusion matrix, model evaluation\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import csr_matrix\n",
        "import nltk\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr6x4WSx2YZz",
        "colab_type": "text"
      },
      "source": [
        "**Mount Gdrive**\n",
        "\n",
        "The gdrive is where the movie reviews data is saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4r_KaV52VkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5a86ce4-d477-4648-9a21-05f577bc06f8"
      },
      "source": [
        "\n",
        "## Mount the gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "## change directry to the gdrive\n",
        "\n",
        "os.chdir('./drive/My Drive/Colab Notebooks/data')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-6x2oLS3Luu",
        "colab_type": "text"
      },
      "source": [
        "**Read the datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID9guQfI21xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "d897de18-9060-42bc-8e84-9f54caa5fcf9"
      },
      "source": [
        "## Read Data\n",
        "\n",
        "train = pd.read_csv('./kaggle-sentiment/train.tsv', sep='\\t')\n",
        "test = pd.read_csv('./kaggle-sentiment/test.tsv', sep='\\t')\n",
        "\n",
        "print('Train shape : {}'.format(train.shape))\n",
        "print('Test shape : {}'.format(test.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9299fe077911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Read Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./kaggle-sentiment/train.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./kaggle-sentiment/test.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./kaggle-sentiment/train.tsv does not exist: './kaggle-sentiment/train.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm05Pbjy9A8R",
        "colab_type": "text"
      },
      "source": [
        "**View Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st0l4q_M4Ofp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egFsVIE39Gux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(train.Sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV0QWCN_9kg8",
        "colab_type": "text"
      },
      "source": [
        "**Get the baseline accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo1XruxG9U7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## count the propotion for each class for baseline accuracy\n",
        "\n",
        "unique, count = np.unique(train.Sentiment, return_counts=True)\n",
        "\n",
        "for clas, i in zip( unique,count):\n",
        "  print('{} : {}%'.format(clas, (i/len(train) * 100)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fatpLjw599JE",
        "colab_type": "text"
      },
      "source": [
        "The baseline accuracy is 51%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDuh2GF7-0pz",
        "colab_type": "text"
      },
      "source": [
        "**Vectorization**\n",
        "\n",
        "MNB: Multinomial Naive Bayes Use count vectorizer as an input. The ngram  count vectorizer willbe used for both MNB and SVM to compared there accuracy in classifying sentiment. SVM did not required any special form of vectorization as input. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BidfD2aQ9vQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
        "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
        "gram13_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,3), min_df=5, stop_words='english')\n",
        "gram23_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(2,3), min_df=5, stop_words='english')\n",
        "gram22_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(2,2), min_df=5, stop_words='english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG4c1TKXCJwn",
        "colab_type": "text"
      },
      "source": [
        "**Get train and target features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbQrBHwaAT7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get the phrase and the sentiment\n",
        "features = train['Phrase'].values\n",
        "target = train['Sentiment'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LHcQrm8CjPY",
        "colab_type": "text"
      },
      "source": [
        "**Cross Validation Score of the model peformance**\n",
        "\n",
        "* create a pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV0A0AGRCdeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## create a pipeline\n",
        "def score_model_pipeline(model, vectorizer, X,y, cv=5):\n",
        "  pipe = Pipeline([('vect', vectorizer), ('model', model)])\n",
        "  scores = cross_val_score(pipe, X,y,cv=cv)\n",
        "  print('Avg Score: {}'.format(sum(scores)/len(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6iLsk0sD30i",
        "colab_type": "text"
      },
      "source": [
        "**Initial Modeling**\n",
        "\n",
        "Cross Val Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZwvHYKEDZQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## create a model container\n",
        "model = {}\n",
        "\n",
        "## add SVM and multinomial naives to the model cointainer\n",
        "model['SVM'] = LinearSVC()\n",
        "model['MNB'] = MultinomialNB()\n",
        "\n",
        "## create a vectorization container\n",
        "vec = {}\n",
        "\n",
        "## add vectorizer to the container\n",
        "vec['ngram12'] =  CountVectorizer(input=\"content\",encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
        "vec['ngram13'] =   CountVectorizer(input=\"content\",encoding='latin-1', ngram_range=(1,3), min_df=5, stop_words='english')\n",
        "vec['ngram23'] = CountVectorizer(input=\"content\",encoding='latin-1', ngram_range=(2,3), min_df=5, stop_words='english')\n",
        "vec['ngram22'] =  CountVectorizer(input=\"content\",encoding='latin-1', ngram_range=(2,2), min_df=5, stop_words='english')\n",
        "\n",
        "\n",
        "\n",
        "## 10 fold cross validation function\n",
        "## create a pipeline\n",
        "def score_model_pipeline(model, vectorizer, X,y, cv=3):\n",
        "  '''10 fold cross validation pipeline and return the average scores'''\n",
        "  nbc = Pipeline([('vect', vectorizer), ('nb', model)])\n",
        "  scores = cross_val_score(nbc, X,y,cv=cv)\n",
        "  print('Avg Score: {}'.format(sum(scores)/len(scores)))\n",
        "  return sum(scores)/len(scores)\n",
        "\n",
        "\n",
        "## get features and label from dataframe\n",
        "def get_X_y_from_df(df, X='text', y='label'):\n",
        "  '''get the text,features and the label, the target. return features and target'''\n",
        "  X = list(df[X].values)\n",
        "  y = df[y].values\n",
        "  return X,y\n",
        "\n",
        "## create an emty list to store score\n",
        "score = []\n",
        "vec_use = []\n",
        "model_use= []\n",
        "\n",
        "## get X and y\n",
        "X,y = get_X_y_from_df(train, X='Phrase', y='Sentiment')\n",
        "\n",
        "## run the cross validation using the pipeline\n",
        "for mod in model:\n",
        "  # loop through the model in the model container\n",
        "  for v in vec:\n",
        "    # loop through the vectorizer in the vectorizer container\n",
        "    cv = score_model_pipeline(model[mod], vec[v], X,y)\n",
        "    score.append(cv)  # append the score\n",
        "    vec_use.append(v)  # append the vectorization used\n",
        "    model_use.append(mod) # append the model used\n",
        "    \n",
        "    \n",
        "  \n",
        "## create a date frame of cross validation score of the classifier\n",
        "result = {'Model': model_use, 'Vectorization': vec_use, '10 fold Avg Score': score}\n",
        "sentiment_df = pd.DataFrame(result)\n",
        "sentiment_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnDUY9f2F8X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1aj-Wm3qx49",
        "colab_type": "text"
      },
      "source": [
        "**Train Test Split**\n",
        "* Create a train and test set. The test set is used to evaluate the model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3VOe6ATq_jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Train test split\n",
        "\n",
        "X_train, X_test, y_train,y_test = train_test_split(features, target, test_size=0.3, random_state=0)\n",
        "\n",
        "print(\"X_train: {}\".format(X_train.shape))\n",
        "print(\"X_test: {}\".format(X_test.shape))\n",
        "print(\"y_train: {}\".format(y_train.shape))\n",
        "print(\"y_test: {}\".format(y_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_1RUIRRqhiO",
        "colab_type": "text"
      },
      "source": [
        "**Data Prep**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFvDPwRZRvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## vectorization\n",
        "\n",
        "X_train_vec = gram12_count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "X_test_vec = gram12_count_vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "## get features names\n",
        "feature_names = gram12_count_vectorizer.get_feature_names()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKIWGLVJtLd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sns.lineplot(sentiment_df.Vectorization,sentiment_df['10 fold Avg Score'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUB9U1i5MDxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#KDEPlot: Kernel Density Estimate Plot\n",
        "fig = plt.figure(figsize=(15,4))\n",
        "ax=sns.kdeplot(sentiment_df.loc[(sentiment_df['Model'] == 'SVM'),'10 fold Avg Score'] , color='b',shade=True, label='SVM')\n",
        "ax=sns.kdeplot(sentiment_df.loc[(sentiment_df['Model'] == 'MNB'),'10 fold Avg Score'] , color='r',shade=True, label='MNB')\n",
        "plt.title('SVM vs MNB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lFLfH_GOLhc",
        "colab_type": "text"
      },
      "source": [
        "**Modeling**\n",
        "\n",
        "### MNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8htkmNmNuvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## MNB\n",
        "mnb_model =  MultinomialNB()\n",
        "\n",
        "mnb_model.fit(X_train_vec, y_train)\n",
        "\n",
        "mnb_prediction = mnb_model.predict(X_test_vec)\n",
        "\n",
        "## Classification Report\n",
        "report = classification_report(mnb_prediction, y_test)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkZQ8Iw4PG3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Confusion matrix\n",
        "mnb_cm  = confusion_matrix(mnb_prediction, y_test)\n",
        "print(mnb_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDRTOl4yQT1w",
        "colab_type": "text"
      },
      "source": [
        "**Feature Importance**: MNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SyjZyi9QIzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# feature importance\n",
        "featLogProb = []\n",
        "ind = 0\n",
        "for feats in feature_names:\n",
        "    ## the following line takes the difference of the log prob of feature given model\n",
        "    ## thus it measure the importance of the feature for classification.\n",
        "    featLogProb.append(abs(mnb_model.feature_log_prob_[1,ind] - mnb_model.feature_log_prob_[0,ind]))\n",
        "    s = \"\"\n",
        "    s += (feats)\n",
        "    s +=\"  \" \n",
        "    s += str(featLogProb[ind])\n",
        "    s += \"\\n\" \n",
        "    print(s)\n",
        "    ind = ind + 1\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0f75NBCRbxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#feats_sorted = sorted(featLogProb , reverse = True)\n",
        "## Sort features based on importance!\n",
        "sort_inds = sorted(range(len(featLogProb)), key=featLogProb.__getitem__, reverse = True)\n",
        "for i in range(10):\n",
        "    s = \"\"\n",
        "    s += feature_names[sort_inds[i]]\n",
        "    s += \":  \"\n",
        "    s += str(featLogProb[sort_inds[i]])\n",
        "    s += \"\\n\"\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xy54F7qR7Ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#feats_sorted = sorted(featLogProb , reverse = True)\n",
        "## Sort features based on importance!\n",
        "sort_inds = sorted(range(len(featLogProb)), key=featLogProb.__getitem__, reverse = False)\n",
        "for i in range(10):\n",
        "    s = \"\"\n",
        "    s += feature_names[sort_inds[i]]\n",
        "    s += \":  \"\n",
        "    s += str(featLogProb[sort_inds[i]])\n",
        "    s += \"\\n\"\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868ZEsRRUC4D",
        "colab_type": "text"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QToXzqAWTpP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the LinearSVC model\n",
        "svm_model = LinearSVC(C=1)\n",
        "\n",
        "\n",
        "# use the training data to train the model\n",
        "svm_model.fit(X_train_vec,y_train)\n",
        "\n",
        "\n",
        "# make prediction\n",
        "svm_prediction = svm_model.predict(X_test_vec)\n",
        "\n",
        "# get classification report\n",
        "print(classification_report(svm_prediction, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d6ODcjEUx2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## print confusion matrix\n",
        "svm_cm = confusion_matrix(svm_prediction, y_test)\n",
        "print(svm_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LHojsdAVOHj",
        "colab_type": "text"
      },
      "source": [
        "**Feature Importtance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr4_acOxVIkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Linear SVC also ranks all features based on their contribution to distinguish the two concepts in each binary classifier\n",
        "## For category \"0\" (very negative), get all features and their weights and sort them in increasing order\n",
        "feature_ranks = sorted(zip(svm_model.coef_[0],gram12_count_vectorizer.get_feature_names()))\n",
        "\n",
        "\n",
        "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
        "very_negative_10 = feature_ranks[-10:]\n",
        "print(\"Very negative words\")\n",
        "for i in range(0, len(very_negative_10)):\n",
        "    print(very_negative_10[i])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6StCdalV2OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
        "not_very_negative_10 = feature_ranks[:10]\n",
        "print(\"not very negative words\")\n",
        "for i in range(0, len(not_very_negative_10)):\n",
        "    print(not_very_negative_10[i])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXmbzK20V989",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Linear SVC also ranks all features based on their contribution to distinguish the two concepts in each binary classifier\n",
        "## For category \"0\" (very negative), get all features and their weights and sort them in increasing order\n",
        "feature_ranks = sorted(zip(svm_model.coef_[4],gram12_count_vectorizer.get_feature_names()))\n",
        "\n",
        "\n",
        "## get the 10 features that are best indicators of very negative sentiment (they are at the bottom of the ranked list)\n",
        "very_positive_10 = feature_ranks[-10:]\n",
        "print(\"Very positive words\")\n",
        "for i in range(0, len(very_positive_10)):\n",
        "    print(very_positive_10[i])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPP1GkDXXtuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get 10 features that are least relevant to \"very negative\" sentiment (they are at the top of the ranked list)\n",
        "not_very_positive_10 = feature_ranks[:10]\n",
        "print(\"not very positive words\")\n",
        "for i in range(0, len(not_very_positive_10)):\n",
        "    print(not_very_positive_10[i])\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCPpg8tMZrVn",
        "colab_type": "text"
      },
      "source": [
        "**Interpretation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmGUD6MDYluK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get the confidence scores for all test examples from each of the five binary classifiers\n",
        "svm_confidence_scores = svm_model.decision_function(X_test_vec)\n",
        "\n",
        "## get the confidence score for the first test example\n",
        "print(svm_confidence_scores[0])\n",
        "\n",
        "## sample output: array([-1.05306321, -0.62746206,  0.31074854, -0.89709483, -1.08343089]\n",
        "## because the confidence score is the highest for category 2, \n",
        "## the prediction should be 2. \n",
        "\n",
        "## Confirm by printing out the actual prediction\n",
        "print(y_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m-ijBmhaHXf",
        "colab_type": "text"
      },
      "source": [
        "**Error Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-Qw1kXYaAd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print out specific type of error for further analysis\n",
        "\n",
        "# print out the very positive examples that are mistakenly predicted as negative\n",
        "# according to the confusion matrix, there should be 5 such examples\n",
        "# note if you use a different vectorizer option, your result might be different\n",
        "\n",
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==4 and svm_prediction[i]==0):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d72WmZerarUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# very positive and its predicted as negative\n",
        "\n",
        "\n",
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==4 and svm_prediction[i]==1):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqp_TPR9c53f",
        "colab_type": "text"
      },
      "source": [
        "**Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSXm8IFebjNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## function for negative detection\n",
        "\n",
        "def has_negation(post):\n",
        "    pattern_neg_1 = re.compile(r'\\b(not|no|never)\\b')\n",
        "    pattern_neg_2 = re.compile(r'\\b([a-z]+less)\\b')\n",
        "    if pattern_neg_1.search(post.lower()) or pattern_neg_2.search(post.lower()):\n",
        "        return 1\n",
        "    else: \n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFkH52JYdazi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txts = ['this is good', 'this is bad', 'this is not good', 'this is not bad', 'this is useless']\n",
        "df = pd.DataFrame({'text':txts})\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_BjssNYeTr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## apply the function\n",
        "\n",
        "df['neg'] = df['text'].apply(lambda x: 1 if has_negation(x) else 0)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfHXhJLTfYuy",
        "colab_type": "text"
      },
      "source": [
        "**Now vectorize the text and combine the word vectors with the negation feature values.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXYkjQAOezzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import sparse\n",
        "\n",
        "vecs = gram12_count_vectorizer.transform(df['text']).astype(float)\n",
        "#print(vecs)\n",
        "X_dense = df[['neg']]\n",
        "X_sparse = vecs\n",
        "X = sparse.hstack([X_sparse, X_dense]).tocsr()\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FTo6EFuhOHc",
        "colab_type": "text"
      },
      "source": [
        "**Now Apply the function to the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPibd3N1f4f8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=train['Sentiment']\n",
        "\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "train['neg'] = train['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = train[['neg']]\n",
        "X_sparse = gram12_count_vectorizer.fit_transform(train['Phrase']).astype(float)\n",
        "X = sparse.hstack([X_sparse, X_dense]).tocsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRyo40RHiOAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# test the model with negation detection\n",
        "\n",
        "svm_model = LinearSVC()\n",
        "scores = cross_val_score(svm_model, X, y, cv=3, n_jobs=3)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi4f6s1QifjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# test the model without negation detection\n",
        "# note this cross validation is not the standard pipeline method\n",
        "# but a cut-corner version that does vectorization first and then train/test models\n",
        "# this cut-corner version would allow the model to see the text of the test data, \n",
        "# but the model would still not see the labels of the test data\n",
        "svm_model2= LinearSVC()\n",
        "scores2 = cross_val_score(svm_model2, X_sparse, y, cv=3, n_jobs=3)\n",
        "avg2=sum(scores2)/len(scores2)\n",
        "print(avg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UaRWK-1j_Tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_model = LinearSVC()\n",
        "\n",
        "svm_model.fit(X,y)\n",
        "\n",
        "########## submit to Kaggle submission\n",
        "\n",
        "\n",
        "# preserve the id column of the test examples\n",
        "kaggle_ids= test['PhraseId'].values\n",
        "\n",
        "# read in the text content of the examples\n",
        "kaggle_X_test=test['Phrase'].values\n",
        "\n",
        "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
        "kaggle_X_test_vec= gram12_count_vectorizer.transform(kaggle_X_test)\n",
        "\n",
        "\n",
        "## add feaure to X_test\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "test['neg'] = test['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = test[['neg']]\n",
        "\n",
        "kaggle_X = sparse.hstack([kaggle_X_test_vec, X_dense]).tocsr()\n",
        "\n",
        "\n",
        "# predict \n",
        "kaggle_pred=svm_model.predict(kaggle_X)\n",
        "\n",
        "# combine the test example ids with their predictions\n",
        "kaggle_submission=zip(kaggle_ids, kaggle_pred)\n",
        "\n",
        "# prepare output file\n",
        "outf=open('kaggle_submission_linearSVC.csv', 'w')\n",
        "\n",
        "# write header\n",
        "outf.write('PhraseId,Sentiment\\n')\n",
        "\n",
        "# write predictions with ids to the output file\n",
        "for x, value in enumerate(kaggle_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
        "\n",
        "# close the output file\n",
        "outf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGufku6DmjZZ",
        "colab_type": "text"
      },
      "source": [
        "**Kaggle test score:** 61.037% "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YM3YDv_mbHp",
        "colab_type": "text"
      },
      "source": [
        "**Model Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w3F8Yo4mS-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "c = [0.1,0.2, 0.3, 0.5, 1, 2,5,7]\n",
        "\n",
        "score = []\n",
        "\n",
        "for i in c:\n",
        "  svm_model = LinearSVC(C=i)\n",
        "  scores = cross_val_score(svm_model, X, y, cv=3, n_jobs=3)\n",
        "  avg=sum(scores)/len(scores)\n",
        "  score.append(avg)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(c,score)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4GhMqOwr2l5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lineplot(c,score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQQhStYviOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "c = [0.001, 0.002, 0.004, 0.005, 0.008, 0.01, 0.02,0.04, 0.1, 0.2, 0.4,0.5,1,2]\n",
        "\n",
        "score = []\n",
        "\n",
        "for i in c:\n",
        "  svm_model = LinearSVC(C=i)\n",
        "  scores = cross_val_score(svm_model, X, y, cv=3, n_jobs=3)\n",
        "  avg=sum(scores)/len(scores)\n",
        "  score.append(avg)\n",
        "  \n",
        "  \n",
        "sns.lineplot(c,score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX4M_SwEzCMz",
        "colab_type": "text"
      },
      "source": [
        "**C=0.2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVfb7QZzwOZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_model = LinearSVC(C=0.2)\n",
        "\n",
        "svm_model.fit(X,y)\n",
        "\n",
        "########## submit to Kaggle submission\n",
        "\n",
        "\n",
        "# preserve the id column of the test examples\n",
        "kaggle_ids= test['PhraseId'].values\n",
        "\n",
        "# read in the text content of the examples\n",
        "kaggle_X_test=test['Phrase'].values\n",
        "\n",
        "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
        "kaggle_X_test_vec= gram12_count_vectorizer.transform(kaggle_X_test)\n",
        "\n",
        "\n",
        "## add feaure to X_test\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "test['neg'] = test['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = test[['neg']]\n",
        "\n",
        "kaggle_X = sparse.hstack([kaggle_X_test_vec, X_dense]).tocsr()\n",
        "\n",
        "\n",
        "# predict \n",
        "kaggle_pred=svm_model.predict(kaggle_X)\n",
        "\n",
        "# combine the test example ids with their predictions\n",
        "kaggle_submission=zip(kaggle_ids, kaggle_pred)\n",
        "\n",
        "# prepare output file\n",
        "outf=open('kaggle_submission_linearSVC-0.2.csv', 'w')\n",
        "\n",
        "# write header\n",
        "outf.write('PhraseId,Sentiment\\n')\n",
        "\n",
        "# write predictions with ids to the output file\n",
        "for x, value in enumerate(kaggle_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
        "\n",
        "# close the output file\n",
        "outf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_XFRlUzlOH",
        "colab_type": "text"
      },
      "source": [
        "**kaggle Test Score**: 60.945%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YeufDph0VNB",
        "colab_type": "text"
      },
      "source": [
        "**C=0.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjx_r3cQzYpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_model = LinearSVC(C=0.1)\n",
        "\n",
        "svm_model.fit(X,y)\n",
        "\n",
        "########## submit to Kaggle submission\n",
        "\n",
        "\n",
        "# preserve the id column of the test examples\n",
        "kaggle_ids= test['PhraseId'].values\n",
        "\n",
        "# read in the text content of the examples\n",
        "kaggle_X_test=test['Phrase'].values\n",
        "\n",
        "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
        "kaggle_X_test_vec= gram12_count_vectorizer.transform(kaggle_X_test)\n",
        "\n",
        "\n",
        "## add feaure to X_test\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "test['neg'] = test['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = test[['neg']]\n",
        "\n",
        "kaggle_X = sparse.hstack([kaggle_X_test_vec, X_dense]).tocsr()\n",
        "\n",
        "\n",
        "# predict \n",
        "kaggle_pred=svm_model.predict(kaggle_X)\n",
        "\n",
        "# combine the test example ids with their predictions\n",
        "kaggle_submission=zip(kaggle_ids, kaggle_pred)\n",
        "\n",
        "# prepare output file\n",
        "outf=open('kaggle_submission_linearSVC-0.2.csv', 'w')\n",
        "\n",
        "# write header\n",
        "outf.write('PhraseId,Sentiment\\n')\n",
        "\n",
        "# write predictions with ids to the output file\n",
        "for x, value in enumerate(kaggle_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
        "\n",
        "# close the output file\n",
        "outf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7XBxB5H1Lxq",
        "colab_type": "text"
      },
      "source": [
        "Kaggle score: 60.366%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1XmBFLM1cS4",
        "colab_type": "text"
      },
      "source": [
        "C=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M9zYwtP0kFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_model = LinearSVC(C=2)\n",
        "\n",
        "svm_model.fit(X,y)\n",
        "\n",
        "########## submit to Kaggle submission\n",
        "\n",
        "\n",
        "# preserve the id column of the test examples\n",
        "kaggle_ids= test['PhraseId'].values\n",
        "\n",
        "# read in the text content of the examples\n",
        "kaggle_X_test=test['Phrase'].values\n",
        "\n",
        "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
        "kaggle_X_test_vec= gram12_count_vectorizer.transform(kaggle_X_test)\n",
        "\n",
        "\n",
        "## add feaure to X_test\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "test['neg'] = test['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = test[['neg']]\n",
        "\n",
        "kaggle_X = sparse.hstack([kaggle_X_test_vec, X_dense]).tocsr()\n",
        "\n",
        "\n",
        "# predict \n",
        "kaggle_pred=svm_model.predict(kaggle_X)\n",
        "\n",
        "# combine the test example ids with their predictions\n",
        "kaggle_submission=zip(kaggle_ids, kaggle_pred)\n",
        "\n",
        "# prepare output file\n",
        "outf=open('kaggle_submission_linearSVC-0.2.csv', 'w')\n",
        "\n",
        "# write header\n",
        "outf.write('PhraseId,Sentiment\\n')\n",
        "\n",
        "# write predictions with ids to the output file\n",
        "for x, value in enumerate(kaggle_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
        "\n",
        "# close the output file\n",
        "outf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MzBV5xz1qgz",
        "colab_type": "text"
      },
      "source": [
        "Score = 60.47%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKVBtLkN2lgK",
        "colab_type": "text"
      },
      "source": [
        "**C=1 has the highest score of 61% on kaggle test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsoUlS3M2xrN",
        "colab_type": "text"
      },
      "source": [
        "**Tune Penalty**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6u-K5zz1ny9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# test the model with negation detection\n",
        "\n",
        "svm_model = LinearSVC(C=1)\n",
        "scores = cross_val_score(svm_model, X, y, cv=3, n_jobs=3)\n",
        "avg=sum(scores)/len(scores)\n",
        "print(avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MHNTEwc48pu",
        "colab_type": "text"
      },
      "source": [
        "## Unigram and Bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywMsVhF73KJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## create a model container\n",
        "model = {}\n",
        "\n",
        "## add SVM and multinomial naives to the model cointainer\n",
        "model['SVM'] = LinearSVC()\n",
        "model['MNB'] = MultinomialNB()\n",
        "\n",
        "## create a vectorization container\n",
        "vec = {}\n",
        "\n",
        "## add vectorizer to the container\n",
        "vec['ngram12'] =  CountVectorizer(input=\"content\",encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english')\n",
        "vec['unigram'] =   CountVectorizer(input=\"content\",encoding='latin-1', binary=False, min_df=5, stop_words='english')\n",
        "\n",
        "\n",
        "## 10 fold cross validation function\n",
        "## create a pipeline\n",
        "def score_model_pipeline(model, vectorizer, X,y, cv=3):\n",
        "  '''10 fold cross validation pipeline and return the average scores'''\n",
        "  nbc = Pipeline([('vect', vectorizer), ('nb', model)])\n",
        "  scores = cross_val_score(nbc, X,y,cv=cv)\n",
        "  print('Avg Score: {}'.format(sum(scores)/len(scores)))\n",
        "  return sum(scores)/len(scores)\n",
        "\n",
        "\n",
        "## get features and label from dataframe\n",
        "def get_X_y_from_df(df, X='text', y='label'):\n",
        "  '''get the text,features and the label, the target. return features and target'''\n",
        "  X = list(df[X].values)\n",
        "  y = df[y].values\n",
        "  return X,y\n",
        "\n",
        "## create an emty list to store score\n",
        "score = []\n",
        "vec_use = []\n",
        "model_use= []\n",
        "\n",
        "## get X and y\n",
        "X,y = get_X_y_from_df(train, X='Phrase', y='Sentiment')\n",
        "\n",
        "## run the cross validation using the pipeline\n",
        "for mod in model:\n",
        "  # loop through the model in the model container\n",
        "  for v in vec:\n",
        "    # loop through the vectorizer in the vectorizer container\n",
        "    cv = score_model_pipeline(model[mod], vec[v], X,y)\n",
        "    score.append(cv)  # append the score\n",
        "    vec_use.append(v)  # append the vectorization used\n",
        "    model_use.append(mod) # append the model used\n",
        "    \n",
        "    \n",
        "  \n",
        "## create a date frame of cross validation score of the classifier\n",
        "result = {'Model': model_use, 'Vectorization': vec_use, '10 fold Avg Score': score}\n",
        "sentiment_df = pd.DataFrame(result)\n",
        "sentiment_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_JME0ae7cam",
        "colab_type": "text"
      },
      "source": [
        "* C =1\n",
        "* bigram\n",
        "* feature engineering\n",
        "* error analysis\n",
        "*SVM(C=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waa5c9FR5l-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_model = LinearSVC(C=1)\n",
        "\n",
        "\n",
        "# use the training data to train the model\n",
        "svm_model.fit(X_train_vec,y_train)\n",
        "\n",
        "\n",
        "# make prediction\n",
        "svm_prediction = svm_model.predict(X_test_vec)\n",
        "\n",
        "# get classification report\n",
        "print(classification_report(svm_prediction, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjXExwp59UVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## print confusion matrix\n",
        "svm_cm = confusion_matrix(svm_prediction, y_test)\n",
        "print(svm_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxPrt_wM9r1Y",
        "colab_type": "text"
      },
      "source": [
        "### **Negative Error Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTBS02_y9p5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==0 and svm_prediction[i]==4):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDMM3nQ699Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==0 and svm_prediction[i]==3):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4eHTyJD-BXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==0 and svm_prediction[i]==2):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRrc7Zfd-JVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==1 and svm_prediction[i]==4):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmj4f8FS-RZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==1 and svm_prediction[i]==3):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2rME59m-fgt",
        "colab_type": "text"
      },
      "source": [
        "**Observation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD7yV1jq_Ylz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_R5AHvz_Y13",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZss4Rra_MlY",
        "colab_type": "text"
      },
      "source": [
        "### **Positive Error Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQKb77da_Q-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==4 and svm_prediction[i]==0):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFMdyXRX_fMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==4 and svm_prediction[i]==1):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pna4dvWb_j2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==4 and svm_prediction[i]==2):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnPUkVzU_mwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==3 and svm_prediction[i]==0):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghld9Dqc_tv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "err_cnt = 0\n",
        "for i in range(0, len(y_test)):\n",
        "    if(y_test[i]==3 and svm_prediction[i]==1):\n",
        "        print(X_test[i])\n",
        "        err_cnt = err_cnt+1\n",
        "print(\"errors:\", err_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsvRZAxu_2-X",
        "colab_type": "text"
      },
      "source": [
        "**Positive Error Observation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wqmQgOa_8Z_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhpKkBV1HF7s",
        "colab_type": "text"
      },
      "source": [
        "**Using nltk stemmer to see if it will improve the performance of the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLF9t2v_xyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## eglish stemmer\n",
        "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "## class to tem and vectorized doc\n",
        "class StemmedCountVectorizer(CountVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
        "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DJpiegUHV74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##  stem vectorizer\n",
        "stem_ngram_vectorizer = StemmedCountVectorizer(input=\"content\",encoding='latin-1', ngram_range=(1,2), min_df=5, stop_words='english', analyzer=\"word\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HUGPFSxHtYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=train['Sentiment']\n",
        "\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "train['neg'] = train['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = train[['neg']]\n",
        "X_sparse = stem_ngram_vectorizer.fit_transform(train['Phrase']).astype(float)\n",
        "X = sparse.hstack([X_sparse, X_dense]).tocsr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTT2Ng53IVQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_model = LinearSVC(C=1)\n",
        "\n",
        "svm_model.fit(X,y)\n",
        "\n",
        "########## submit to Kaggle submission\n",
        "\n",
        "\n",
        "# preserve the id column of the test examples\n",
        "kaggle_ids= test['PhraseId'].values\n",
        "\n",
        "# read in the text content of the examples\n",
        "kaggle_X_test=test['Phrase'].values\n",
        "\n",
        "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
        "kaggle_X_test_vec= stem_ngram_vectorizer.transform(kaggle_X_test)\n",
        "\n",
        "\n",
        "## add feaure to X_test\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "test['neg'] = test['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = test[['neg']]\n",
        "\n",
        "kaggle_X = sparse.hstack([kaggle_X_test_vec, X_dense]).tocsr()\n",
        "\n",
        "\n",
        "# predict \n",
        "kaggle_pred=svm_model.predict(kaggle_X)\n",
        "\n",
        "# combine the test example ids with their predictions\n",
        "kaggle_submission=zip(kaggle_ids, kaggle_pred)\n",
        "\n",
        "# prepare output file\n",
        "outf=open('kaggle_submission_linearSVC-stem.csv', 'w')\n",
        "\n",
        "# write header\n",
        "outf.write('PhraseId,Sentiment\\n')\n",
        "\n",
        "# write predictions with ids to the output file\n",
        "for x, value in enumerate(kaggle_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
        "\n",
        "# close the output file\n",
        "outf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSxhXFRcJv-8",
        "colab_type": "text"
      },
      "source": [
        "Kaggle Test Score = 60.7%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O850e0pnLm-a",
        "colab_type": "text"
      },
      "source": [
        "### **Highest Test Score Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlMV3qyQI6Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=train['Sentiment']\n",
        "\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "train['neg'] = train['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = train[['neg']]\n",
        "X_sparse = gram12_count_vectorizer.fit_transform(train['Phrase']).astype(float)\n",
        "X = sparse.hstack([X_sparse, X_dense]).tocsr()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "svm_model = LinearSVC()\n",
        "\n",
        "svm_model.fit(X,y)\n",
        "\n",
        "########## submit to Kaggle submission\n",
        "\n",
        "\n",
        "# preserve the id column of the test examples\n",
        "kaggle_ids= test['PhraseId'].values\n",
        "\n",
        "# read in the text content of the examples\n",
        "kaggle_X_test=test['Phrase'].values\n",
        "\n",
        "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
        "kaggle_X_test_vec= gram12_count_vectorizer.transform(kaggle_X_test)\n",
        "\n",
        "\n",
        "## add feaure to X_test\n",
        "pattern_neg = re.compile(r'\\b(not|no|never)\\b')\n",
        "test['neg'] = test['Phrase'].apply(lambda x: 1 if pattern_neg.search(x.lower()) else 0)\n",
        "\n",
        "X_dense = test[['neg']]\n",
        "\n",
        "kaggle_X = sparse.hstack([kaggle_X_test_vec, X_dense]).tocsr()\n",
        "\n",
        "\n",
        "# predict \n",
        "kaggle_pred=svm_model.predict(kaggle_X)\n",
        "\n",
        "# combine the test example ids with their predictions\n",
        "kaggle_submission=zip(kaggle_ids, kaggle_pred)\n",
        "\n",
        "# prepare output file\n",
        "outf=open('kaggle_submission_linearSVC-final.csv', 'w')\n",
        "\n",
        "# write header\n",
        "outf.write('PhraseId,Sentiment\\n')\n",
        "\n",
        "# write predictions with ids to the output file\n",
        "for x, value in enumerate(kaggle_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
        "\n",
        "# close the output file\n",
        "outf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXiAnz-9PzXp",
        "colab_type": "text"
      },
      "source": [
        "Score: 61.037%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1-gQIkMMGfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}